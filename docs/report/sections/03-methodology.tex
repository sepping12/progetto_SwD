\section{Methodology}
\label{sec:methodology}

This chapter describes the experimental methodology employed in this dependability analysis, including the initial state assessment, evaluation procedures, and success criteria for each of the nine evaluation criteria.

\subsection{Experimental Environment}

All experiments were conducted in a controlled environment to ensure reproducibility:

\begin{table}[htbp]
\centering
\caption{Experimental Environment Specifications}
\label{tab:environment}
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Specification} \\
\midrule
Operating System (Local) & Windows 11 \\
Operating System (CI) & Ubuntu 22.04 (GitHub Actions) \\
Java Version & Eclipse Temurin 17 (LTS) \\
Build Tool & Apache Maven 3.9.x (Maven Wrapper) \\
IDE & Visual Studio Code with Java extensions \\
Version Control & Git 2.x \\
Container Runtime & Docker Desktop 4.x \\
Database (Production) & MySQL 8.0 \\
Database (Testing) & H2 2.x (in-memory) \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Initial State Assessment}

Before implementing improvements, the application's initial state was documented:

\begin{table}[htbp]
\centering
\caption{Application Initial State}
\label{tab:initial-state}
\begin{tabular}{lll}
\toprule
\textbf{Aspect} & \textbf{Initial State} & \textbf{Notes} \\
\midrule
CI/CD Pipeline & Basic GitHub Actions & Build only \\
Static Analysis & Not integrated & No SonarCloud \\
Docker Configuration & Basic Dockerfile & No multi-stage \\
Test Suite Size & 89 tests & Original tests \\
Code Coverage & Unknown & No JaCoCo \\
Mutation Testing & Not implemented & No PITest \\
Performance Benchmarks & None & No JMH \\
Security Scanning & None & No OWASP/SpotBugs \\
Generated Tests & None & No Randoop \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Tool Selection Rationale}

Tools were selected based on industry adoption, academic validation, and Spring Boot compatibility:

\begin{table}[htbp]
\centering
\caption{Selected Tools and Rationale}
\label{tab:tools}
\begin{tabular}{lll}
\toprule
\textbf{Tool} & \textbf{Purpose} & \textbf{Rationale} \\
\midrule
SonarCloud & Static analysis & Industry standard, free for open source \\
JaCoCo 0.8.10 & Coverage & Maven ecosystem integration \\
PITest 1.14.4 & Mutation testing & Most mature Java mutation testing \\
JMH 1.37 & Performance & OpenJDK official benchmark framework \\
Randoop 4.3.3 & Test generation & Academic and industry proven \\
OWASP DC 9.0.7 & Dependency security & NIST NVD integration \\
SpotBugs 4.8.3 & Bug detection & FindBugs successor \\
FindSecBugs 1.12.0 & Security patterns & OWASP security focus \\
Docker & Containerization & Industry standard \\
GitHub Actions & CI/CD & Native GitHub integration \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Criteria Methodology}

\subsubsection{Criterion 1: CI/CD Pipeline}

\textbf{Objective}: Establish automated build, test, and deployment pipeline

\textbf{Method}:
\begin{enumerate}
    \item Design GitHub Actions workflow structure
    \item Implement build workflow with Maven
    \item Add test execution and reporting
    \item Configure JaCoCo coverage reporting
    \item Integrate SonarCloud analysis
    \item Set up Docker image build and push
    \item Configure workflow triggers (push, pull request, schedule)
\end{enumerate}

\textbf{Success Metrics}:
\begin{itemize}
    \item Green build status on main branch
    \item All tests passing automatically
    \item Coverage reports generated
    \item Docker image pushed to registry
    \item Build time under 60 seconds
\end{itemize}

\subsubsection{Criterion 2: Static Code Analysis (SonarCloud)}

\textbf{Objective}: Assess code quality via automated static analysis

\textbf{Method}:
\begin{enumerate}
    \item Create SonarCloud account and project
    \item Configure \texttt{sonar-project.properties}
    \item Integrate SonarCloud with GitHub Actions
    \item Execute initial analysis
    \item Review and categorize issues
    \item Document findings with rationale
\end{enumerate}

\textbf{Success Metrics}:
\begin{itemize}
    \item Quality Gate: PASSED
    \item Security Rating: A
    \item Reliability Rating: A
    \item Maintainability Rating: A
    \item Code Coverage: > 80\%
\end{itemize}

\subsubsection{Criterion 3 \& 4: Containerization}

\textbf{Objective}: Create production-ready Docker deployment

\textbf{Method}:
\begin{enumerate}
    \item Design multi-stage Dockerfile
    \item Implement builder stage (Maven compilation)
    \item Implement runtime stage (JRE only)
    \item Add health check configuration
    \item Optimize image size and layers
    \item Test container locally
    \item Push image to DockerHub
    \item Verify public accessibility
\end{enumerate}

\textbf{Success Metrics}:
\begin{itemize}
    \item Container runs successfully
    \item Application accessible on localhost:8080
    \item Health check endpoint responsive
    \item Image size optimized (< 300MB)
    \item Multi-stage build working
\end{itemize}

\subsubsection{Criterion 5: Test Coverage (JaCoCo)}

\textbf{Objective}: Measure code coverage comprehensively

\textbf{Method}:
\begin{enumerate}
    \item Configure JaCoCo Maven plugin
    \item Set coverage thresholds (80\% line, 75\% branch)
    \item Execute test suite with coverage tracking
    \item Generate HTML and XML reports
    \item Analyze coverage by package and class
    \item Identify uncovered critical paths
    \item Document coverage gaps
\end{enumerate}

\textbf{Calculation}:
\begin{equation}
\text{Line Coverage} = \frac{\text{Lines Executed}}{\text{Total Lines}} \times 100\%
\end{equation}

\textbf{Success Metrics}:
\begin{itemize}
    \item Overall coverage: > 80\%
    \item Service layer: > 90\%
    \item Controller layer: > 85\%
    \item Report generated successfully
\end{itemize}

\subsubsection{Criterion 6: Mutation Testing (PITest)}

\textbf{Objective}: Evaluate test suite effectiveness

\textbf{Method}:
\begin{enumerate}
    \item Configure PITest Maven plugin
    \item Define target classes (service, controller, DTO)
    \item Execute mutation campaign (10--15 minutes)
    \item Analyze mutation operators and results
    \item Review survived mutants
    \item Identify test weaknesses
    \item Implement edge case tests
    \item Configure Lombok method exclusion
    \item Re-run mutation testing
    \item Verify 100\% business logic coverage
\end{enumerate}

\textbf{Calculation}:
\begin{equation}
\text{Mutation Score} = \frac{\text{Killed Mutants}}{\text{Total Mutants} - \text{NO\_COVERAGE}} \times 100\%
\end{equation}

\textbf{Success Metrics}:
\begin{itemize}
    \item Mutation score: > 80\%
    \item Test strength: > 95\%
    \item No survived mutants in critical paths
    \item Business logic: 100\% coverage
\end{itemize}

\subsubsection{Criterion 7: Performance Testing (JMH)}

\textbf{Objective}: Establish performance baseline

\textbf{Method}:
\begin{enumerate}
    \item Add JMH dependencies to \texttt{pom.xml}
    \item Identify critical operations for benchmarking
    \item Design benchmark tests:
        \begin{itemize}
            \item CheckoutService operations
            \item UUID generation
            \item Entity operations (Customer, Order)
            \item DTO operations (Purchase)
        \end{itemize}
    \item Configure JMH parameters:
        \begin{itemize}
            \item Warmup: 3 iterations
            \item Measurement: 5 iterations
            \item Fork: 1 (separate JVM)
        \end{itemize}
    \item Execute benchmarks
    \item Analyze results (throughput, latency)
    \item Document performance characteristics
\end{enumerate}

\textbf{Success Metrics}:
\begin{itemize}
    \item All operations < 1ms average
    \item Throughput > 1M ops/sec for simple operations
    \item No memory leaks detected
    \item Consistent performance across runs
\end{itemize}

\subsubsection{Criterion 8: Automated Test Generation (Randoop)}

\textbf{Objective}: Supplement test suite with generated tests

\textbf{Method}:
\begin{enumerate}
    \item Download Randoop 4.3.3 JAR
    \item Identify target classes (entities, DTOs)
    \item Configure generation parameters:
        \begin{itemize}
            \item Time limit: 60 seconds
            \item Output: JUnit 4 tests
        \end{itemize}
    \item Execute test generation
    \item Review generated tests
    \item Add JUnit Vintage Engine for compatibility
    \item Integrate tests into Maven build
    \item Measure coverage improvement
    \item Document generation process
\end{enumerate}

\textbf{Success Metrics}:
\begin{itemize}
    \item > 500 tests generated
    \item 100\% generated tests passing
    \item Coverage improvement documented
    \item Tests integrated in CI/CD
\end{itemize}

\subsubsection{Criterion 9: Security Analysis}

\textbf{Objective}: Identify and address security vulnerabilities

\textbf{Method}:
\begin{enumerate}
    \item Configure SpotBugs with FindSecBugs plugin
    \item Configure OWASP Dependency-Check
    \item Execute SpotBugs security scan
    \item Execute OWASP dependency scan
    \item Review SonarCloud security findings
    \item Categorize vulnerabilities by severity
    \item Assess OWASP Top 10 compliance
    \item Propose mitigation strategies
    \item Document security posture
\end{enumerate}

\textbf{Success Metrics}:
\begin{itemize}
    \item Zero critical vulnerabilities
    \item Zero high-severity issues
    \item All dependencies up-to-date
    \item Security rating: A
    \item OWASP Top 10 compliant
\end{itemize}

\subsection{Data Collection}

For each criterion, the following data was systematically collected:

\begin{itemize}
    \item \textbf{Quantitative Metrics}: Coverage percentages, mutation scores, performance measurements
    \item \textbf{Tool Outputs}: HTML reports, XML data, log files
    \item \textbf{Screenshots}: Dashboard views, CI/CD status, report summaries
    \item \textbf{Configuration Files}: Maven POM, workflow YAML, Dockerfile
    \item \textbf{Timestamps}: Build durations, test execution times, analysis times
\end{itemize}

\subsection{Iterative Improvement Process}

The analysis followed an iterative methodology:

\begin{enumerate}
    \item \textbf{Baseline Measurement}: Establish initial metrics
    \item \textbf{Analysis}: Identify issues and improvement opportunities
    \item \textbf{Implementation}: Apply fixes and enhancements
    \item \textbf{Re-measurement}: Verify improvements
    \item \textbf{Documentation}: Record changes and rationale
    \item \textbf{Iteration}: Repeat until targets achieved
\end{enumerate}

\subsection{Quality Assurance}

To ensure analysis validity:

\begin{itemize}
    \item \textbf{Reproducibility}: All analyses executable via documented commands
    \item \textbf{Version Control}: All changes committed to Git with descriptive messages
    \item \textbf{Automated Testing}: CI/CD pipeline validates all changes
    \item \textbf{Peer Review}: Code reviews via pull requests
    \item \textbf{Documentation}: Comprehensive README and analysis reports
\end{itemize}
