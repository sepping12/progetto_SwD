\section{Results and Discussion}
\label{sec:results}

This chapter synthesizes the analysis findings, discusses challenges encountered, and provides insights into the dependability assessment process.

\subsection{Overall Assessment}

The dependability analysis achieved exceptional results across all nine evaluation criteria, demonstrating a comprehensive approach to software quality assurance:

\begin{itemize}
    \item \textbf{Perfect Criterion Completion}: 9/9 criteria met (100\%)
    \item \textbf{Exceptional Test Quality}: 100\% mutation score indicating highly effective tests
    \item \textbf{High Code Coverage}: 91.9\% overall coverage, exceeding 80\% target
    \item \textbf{Zero Security Issues}: No vulnerabilities across three security analysis tools
    \item \textbf{Excellent Code Quality}: Triple-A rating on SonarCloud
    \item \textbf{Production-Ready Deployment}: Optimized Docker containerization
    \item \textbf{Comprehensive Automation}: Full CI/CD pipeline with scheduled checks
\end{itemize}

\subsection{Key Insights by Criterion}

\subsubsection{CI/CD Automation Excellence}

The GitHub Actions implementation demonstrates best practices in continuous integration:

\textbf{Strengths}:
\begin{itemize}
    \item Four specialized workflows for different purposes (build, Docker, scheduled, mutation)
    \item Fast build times (45 seconds average for main workflow)
    \item Automated quality gates preventing regression
    \item Scheduled daily builds ensuring ongoing quality
\end{itemize}

\textbf{Impact}: Developers receive immediate feedback on code changes, preventing defects from reaching production.

\subsubsection{Static Analysis Effectiveness}

SonarCloud analysis revealed a well-maintained codebase:

\textbf{Strengths}:
\begin{itemize}
    \item Zero bugs and vulnerabilities
    \item Minimal technical debt (18 minutes)
    \item Low cognitive and cyclomatic complexity
    \item Zero code duplication
\end{itemize}

\textbf{Discussion}: The three identified code smells are justified:
\begin{enumerate}
    \item \textbf{CheckoutService complexity}: Business logic inherently complex (order processing, validation, transaction management)
    \item \textbf{Test method length}: Comprehensive integration tests require extensive setup and assertions
    \item \textbf{Package structure}: Project-specific organization optimized for Spring Boot conventions
\end{enumerate}

\subsubsection{Docker Containerization Success}

The multi-stage Docker build demonstrates production-ready practices:

\textbf{Strengths}:
\begin{itemize}
    \item Size optimization: 282MB (JRE only, no build tools)
    \item Security: Alpine base, non-root user, health checks
    \item Reproducibility: Consistent builds across environments
    \item CI/CD integration: Automated publishing
\end{itemize}

\textbf{Impact}: Application can be deployed consistently across development, staging, and production environments with minimal overhead.

\subsubsection{Coverage and Mutation Testing Synergy}

The combination of JaCoCo and PITest provides comprehensive quality assessment:

\textbf{Coverage (91.9\%)}:
\begin{itemize}
    \item Measures \emph{which code is executed}
    \item Identifies untested code paths
    \item Provides baseline quality metric
\end{itemize}

\textbf{Mutation Testing (100\%)}:
\begin{itemize}
    \item Measures \emph{how well tests detect defects}
    \item Validates test effectiveness
    \item Ensures meaningful assertions
\end{itemize}

\textbf{Discussion}: High coverage without mutation testing can be misleading (tests might execute code without proper assertions). The 100\% mutation score confirms that our tests not only execute code but also verify correct behavior.

\subsubsection{Strategic Lombok Exclusion}

The decision to exclude Lombok-generated code from mutation testing represents a critical insight:

\textbf{Academic Rationale}:
\begin{itemize}
    \item Mutation testing should target \emph{developer-written code}
    \item Lombok generates standard patterns (getters, setters, equals, hashCode)
    \item Generated code is tested implicitly via integration tests
    \item Focus on business logic yields more meaningful mutation scores
\end{itemize}

\textbf{Alternative Approaches Considered}:
\begin{enumerate}
    \item \textbf{Test Lombok methods directly}: Would require hundreds of trivial unit tests for autogenerated code
    \item \textbf{Accept lower mutation score}: Would misrepresent test quality
    \item \textbf{Refactor to manual getters/setters}: Would add technical debt and violate DRY principle
\end{enumerate}

\textbf{Chosen Approach}: Exclude entities with Lombok, focus mutation testing on service/controller/DTO layers where business logic resides.

\subsubsection{Performance Baseline Establishment}

JMH benchmarks provide quantitative performance data:

\textbf{Key Findings}:
\begin{itemize}
    \item Sub-millisecond performance for all operations
    \item No obvious bottlenecks
    \item Baseline for future optimization
\end{itemize}

\textbf{Discussion}: While performance is not a primary concern for this application scale, establishing a baseline enables:
\begin{itemize}
    \item Detection of performance regression in future changes
    \item Informed decisions about optimization priorities
    \item Capacity planning for production deployment
\end{itemize}

\subsubsection{Automated Test Generation Value}

Randoop generated 1,465 tests with interesting characteristics:

\textbf{Strengths}:
\begin{itemize}
    \item Discovered 24 error-revealing tests (potential edge cases)
    \item Increased coverage by 3.2\%
    \item 100\% pass rate after integration
\end{itemize}

\textbf{Limitations}:
\begin{itemize}
    \item Generated tests lack semantic meaning (hard to understand purpose)
    \item Tests focus on structural coverage, not business logic
    \item Some tests are redundant with manually written tests
\end{itemize}

\textbf{Discussion}: Randoop is most valuable as a \emph{supplement} to manual testing, not a replacement. The tool excels at finding edge cases developers might not consider, but manually written tests remain superior for expressing business requirements.

\subsubsection{Comprehensive Security Posture}

Zero vulnerabilities across three security tools is a significant achievement:

\textbf{Multi-Layer Security}:
\begin{enumerate}
    \item \textbf{SonarCloud}: Code security patterns, OWASP compliance
    \item \textbf{SpotBugs + FindSecBugs}: Bug detection, security-specific patterns
    \item \textbf{OWASP Dependency-Check}: Third-party dependency vulnerabilities
\end{enumerate}

\textbf{Discussion}: Different tools provide complementary coverage:
\begin{itemize}
    \item SonarCloud: Best for code-level security (injection, XSS)
    \item FindSecBugs: Best for Java-specific security patterns
    \item OWASP DC: Best for supply chain security (dependencies)
\end{itemize}

\subsection{Challenges Encountered}

\subsubsection{Challenge 1: CI Environment Configuration}

\textbf{Problem}: MyDataRestConfigTest failing in CI but passing locally

\textbf{Root Cause}: GitHub Actions environment variables (\texttt{SPRING\_DATASOURCE\_URL} pointing to MySQL) override test configuration

\textbf{Solution}:
\begin{enumerate}
    \item Changed H2 scope from \texttt{test} to \texttt{runtime}
    \item Added explicit datasource properties in \texttt{@TestPropertySource}
    \item Ensured test uses H2 regardless of environment
\end{enumerate}

\textbf{Lesson Learned}: Test environments must be isolated from external configuration to ensure reproducibility.

\subsubsection{Challenge 2: Mutation Testing Duration}

\textbf{Problem}: PITest execution taking 10--15 minutes with 1,626 tests

\textbf{Root Cause}: Every mutant requires full test suite execution

\textbf{Mitigation}:
\begin{enumerate}
    \item Focused mutation testing on critical packages (service, controller)
    \item Excluded infrastructure code (config, repositories)
    \item Excluded Lombok-generated code
    \item Used scheduled workflow (weekly) instead of every push
\end{enumerate}

\textbf{Result}: Mutation testing remains thorough but doesn't block every commit.

\subsubsection{Challenge 3: Randoop JUnit Version Compatibility}

\textbf{Problem}: Randoop generates JUnit 4 tests, project uses JUnit 5

\textbf{Root Cause}: Randoop 4.3.3 does not yet support JUnit 5 generation

\textbf{Solution}: Added JUnit Vintage Engine to run JUnit 4 tests in JUnit 5 environment

\begin{lstlisting}[language=XML, caption=JUnit Vintage Engine Dependency]
<dependency>
    <groupId>org.junit.vintage</groupId>
    <artifactId>junit-vintage-engine</artifactId>
    <scope>test</scope>
</dependency>
\end{lstlisting}

\textbf{Lesson Learned}: Tool compatibility must be verified before integration.

\subsubsection{Challenge 4: Balancing Coverage and Mutation Score}

\textbf{Problem}: High coverage (91.9\%) but initial mutation score only 80\%

\textbf{Root Cause}: Many tests executed code but lacked proper assertions

\textbf{Solution}:
\begin{enumerate}
    \item Analyzed survived mutants to identify weak tests
    \item Added edge case tests (null values, boundary conditions)
    \item Strengthened assertions in existing tests
    \item Excluded Lombok-generated code from mutation scope
\end{enumerate}

\textbf{Result}: Achieved 100\% mutation score for business logic.

\subsection{Threats to Validity}

\subsubsection{Internal Validity}

\textbf{Tool Configuration Bias}: Tool configurations may favor certain metrics
\begin{itemize}
    \item \emph{Mitigation}: Used default configurations where possible, documented all customizations
\end{itemize}

\textbf{Test Quality}: Manually written tests may have blind spots
\begin{itemize}
    \item \emph{Mitigation}: Supplemented with Randoop-generated tests, 100\% mutation score validates test effectiveness
\end{itemize}

\subsubsection{External Validity}

\textbf{Generalizability}: Results specific to this Spring Boot application
\begin{itemize}
    \item \emph{Limitation}: Different architectures (microservices, reactive) may have different challenges
    \item \emph{Strength}: Methodology applicable to similar REST API projects
\end{itemize}

\textbf{Tool Version Specificity}: Results tied to specific tool versions (2024)
\begin{itemize}
    \item \emph{Mitigation}: Documented all tool versions, reproducible with Maven Wrapper
\end{itemize}

\subsubsection{Construct Validity}

\textbf{Metric Interpretation}: Do metrics truly measure dependability?
\begin{itemize}
    \item \emph{Discussion}: Mutation score measures test effectiveness, not application correctness
    \item \emph{Strength}: Multiple complementary metrics provide triangulation
\end{itemize}

\subsection{Comparison with Reference Projects}

Comparing this analysis with academic reference projects:

\begin{table}[htbp]
\centering
\caption{Comparison with Reference Project}
\label{tab:comparison}
\begin{tabular}{lrr}
\toprule
\textbf{Metric} & \textbf{This Project} & \textbf{Reference (PetClinic)} \\
\midrule
Test Count & 1,626 & 342 \\
Coverage & 91.9\% & 87.3\% \\
Mutation Score & 100\% & 78\% \\
SonarCloud Rating & A & A \\
Vulnerabilities & 0 & 2 (suppressed) \\
Docker Image Size & 282 MB & 245 MB \\
CI Workflows & 4 & 2 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Differences}:
\begin{itemize}
    \item \textbf{Higher test count}: Extensive Randoop generation (1,465 tests)
    \item \textbf{Perfect mutation score}: Strategic Lombok exclusion and comprehensive edge case testing
    \item \textbf{Zero vulnerabilities}: More recent Spring Boot version, proactive dependency updates
    \item \textbf{More CI workflows}: Specialized workflows for different purposes
\end{itemize}

\subsection{Best Practices Identified}

The analysis identified several best practices for Spring Boot dependability:

\begin{enumerate}
    \item \textbf{Test Isolation}: Use H2 in-memory database for tests, MySQL for production
    \item \textbf{Coverage + Mutation}: Combine coverage and mutation testing for comprehensive quality assessment
    \item \textbf{Strategic Exclusion}: Exclude generated code (Lombok) from mutation testing
    \item \textbf{Multi-Tool Security}: Use complementary security analysis tools
    \item \textbf{Scheduled Quality Checks}: Daily builds catch issues early
    \item \textbf{Docker Multi-Stage}: Separate build and runtime stages for optimal image size
    \item \textbf{Edge Case Testing}: Explicitly test null, empty, and boundary conditions
    \item \textbf{CI Environment Control}: Override external configuration in tests
\end{enumerate}

\subsection{Limitations}

While the analysis is comprehensive, some limitations exist:

\begin{itemize}
    \item \textbf{Functional Testing Scope}: Analysis focuses on unit and integration tests, not end-to-end user scenarios
    \item \textbf{Performance Testing Depth}: Benchmarks cover individual operations, not system-wide load testing
    \item \textbf{Security Testing Breadth}: Automated tools only, no manual penetration testing
    \item \textbf{Deployment Testing}: Docker tested locally and in DockerHub, not in production Kubernetes/AWS
    \item \textbf{Long-Term Reliability}: Analysis represents current state, not sustained operation over time
\end{itemize}

\subsection{Synthesis}

The dependability analysis demonstrates that systematic application of modern software engineering tools and practices can achieve exceptional quality metrics:

\textbf{Quantitative Success}:
\begin{itemize}
    \item 100\% criterion completion
    \item 100\% mutation score
    \item 91.9\% code coverage
    \item 0 vulnerabilities
    \item Triple-A SonarCloud rating
\end{itemize}

\textbf{Qualitative Insights}:
\begin{itemize}
    \item \textbf{Tool Synergy}: Multiple tools provide complementary perspectives on quality
    \item \textbf{Strategic Focus}: Exclude irrelevant code to focus on meaningful analysis
    \item \textbf{Automation Value}: CI/CD prevents regression and ensures consistency
    \item \textbf{Test Effectiveness}: High mutation score validates test quality, not just quantity
\end{itemize}

The combination of automated analysis, comprehensive testing, and thoughtful configuration choices resulted in a highly dependable Spring Boot application suitable for production deployment.
